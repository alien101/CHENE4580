{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqKvMuKEtapcmvZOu84NJB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"uGwsEPSA-ouf","executionInfo":{"status":"ok","timestamp":1666579127541,"user_tz":240,"elapsed":4,"user":{"displayName":"Nerissa Lien","userId":"11566914959160538996"}}},"outputs":[],"source":["# libraries\n","from google.colab import drive\n","import os\n","import numpy as np"]},{"cell_type":"markdown","source":["#Problem 1\n","### To measure the distance between upper and lower upper planes, we need to compute the distance from x0 to one of the plane times 2. \n","\n","#### Upper: wTx + b = 1  x+\n","#### Lower: wTx + b = -1 x-\n","#### w is the normal vector of the hyperplane\n","#### w/ ||w|| is the unit normal vector\n","#### margin = ((x+) - (x-)) w/ ||w||\n","#### -> ((x+)w - (x-)w)/ ||w||\n","#### -> (1 - b + 1 + b)/ ||w||\n","#### -> 2/ ||w||\n","\n"],"metadata":{"id":"Q-Tzv99S_uwD"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n","# use svm classification\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LinearRegression, ElasticNet\n","from sklearn.metrics import silhouette_samples, silhouette_score, f1_score, confusion_matrix\n","from sklearn.neighbors import NearestNeighbors\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm"],"metadata":{"id":"qp4PLBMwcxec","executionInfo":{"status":"ok","timestamp":1666579128258,"user_tz":240,"elapsed":720,"user":{"displayName":"Nerissa Lien","userId":"11566914959160538996"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Problem 2\n","# 2.a import data \n","drive.mount('/content/gdrive')\n","os.chdir('/content/gdrive/MyDrive/Colab Notebooks/hw6')\n","Xtrain = np.loadtxt('hw6_data_train.csv', delimiter=',')\n","ytrain = np.loadtxt('hw6_data_train_labels.csv', delimiter=',')\n","Xtest = np.loadtxt('hw6_data_test.csv', delimiter=',')\n","ytest = np.loadtxt('hw6_data_test_labels.csv', delimiter=',')\n","print('Xtrain shape: %s\\nytrain shape: %s\\nXtest shape: %s\\nytest shape: %s'%(Xtrain.shape, ytrain.shape, Xtest.shape, ytest.shape))"],"metadata":{"id":"YVWOUrfU_s7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.b kmean and hierarchy clustering: determining parameters by looping\n","n = 100\n","k_ss = []\n","h_ss = []\n","for i in range(n):\n","  kmean_model = KMeans(n_clusters = i + 2, random_state=0)\n","  hier_model = AgglomerativeClustering(n_clusters = i + 2)\n","\n","  k_labels = kmean_model.fit_predict(Xtrain)\n","  h_labels = hier_model.fit_predict(Xtrain)\n","\n","  k_s = silhouette_score(Xtrain, k_labels)\n","  h_s = silhouette_score(Xtrain, h_labels)\n","\n","  k_ss.append(k_s)\n","  h_ss.append(h_s)\n"],"metadata":{"id":"-qtQjENyHDtf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.b DBSCAN: determining parameters by looping\n","d_ss = []\n","max_e = 0\n","max_min_s = 0\n","max_ds = 0\n","for e in np.linspace(0.01,1,100):\n","  for min_s in np.linspace(5, 20):\n","    dbscan_model = DBSCAN(eps = e, min_samples = min_s)\n","    d_labels = dbscan_model.fit_predict(Xtrain)\n","    try:\n","      d_s = silhouette_score(Xtrain, d_labels)\n","      if d_s > max_ds:\n","        max_e = e \n","        max_min_s = min_s\n","        max_ds = d_s\n","    except ValueError:\n","         continue"],"metadata":{"id":"mRzRdO5wTVa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.b DBSCAN: determining parameters using elbow -> doesn't work \n","neighbors = NearestNeighbors(n_neighbors=int(max_min_s))\n","neighbors_fit = neighbors.fit(Xtrain)\n","distances, indices = neighbors_fit.kneighbors(Xtrain)\n","distances = np.sort(distances, axis=0)\n","distances = distances[:,1]\n","plt.plot(distances)"],"metadata":{"id":"VrRpIJptHzvn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#max_e = 0.04\n","print(\"e: %.4f, min_s: %d\" %(max_e, max_min_s))\n","print(\"best k: %d\" %(np.argmax(k_ss) + 2))\n","print(\"best # of clusters for hierarchy: %d\" %(np.argmax(h_ss) + 2))"],"metadata":{"id":"DeWepEOOu_Wq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.b Visualize performance in each clustering\n","fig, ax = plt.subplots(3,2)\n","\n","fig.set_size_inches(30, 40)\n","# n_clusters of each model\n","k = 14\n","h = 22\n","e = 0.13\n","min_s = 5\n","\n","\n","kmean_model = KMeans(n_clusters = k, random_state=0)\n","best_k_labels = kmean_model.fit_predict(Xtrain, ytrain)\n","\n","hier_model = AgglomerativeClustering(n_clusters = h)\n","best_h_labels = hier_model.fit_predict(Xtrain)\n","\n","dbscan_model = DBSCAN(eps = e, min_samples = min_s).fit(Xtrain)\n","best_d_labels = dbscan_model.labels_\n","d = len(set(best_d_labels)) - (1 if -1 in best_d_labels else 0)\n","\n","ax[0,0].set_xlim([0, 1])\n","ax[0,0].set_ylim([0, len(Xtrain) + (k + 1) * 10])\n","\n","ax[1,0].set_xlim([0, 1])\n","ax[1,0].set_ylim([0, len(Xtrain) + (h + 1) * 10])\n","\n","ax[2,0].set_xlim([0, 1])\n","ax[2,0].set_ylim([1, len(Xtrain) + (d + 1) * 10])\n","\n","k_sample_silhouette_values = silhouette_samples(Xtrain, best_k_labels)\n","h_sample_silhouette_values = silhouette_samples(Xtrain, best_h_labels)\n","d_sample_silhouette_values = silhouette_samples(Xtrain, best_d_labels)\n","\n","# source cite: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n","y_lower = 4\n","for i in range(k):\n","    clus_i = k_sample_silhouette_values[best_k_labels == i]\n","\n","    clus_i.sort()\n","\n","    size_cluster_i = clus_i.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = cm.nipy_spectral(float(i) / k)\n","    ax[0,0].fill_betweenx(\n","        np.arange(y_lower, y_upper),\n","        0,\n","        clus_i,\n","        facecolor=color,\n","        edgecolor=color,\n","        alpha=0.7,\n","    )\n","    ax[0,0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","    y_lower = y_upper + 10\n","\n","ax[0,0].axvline(x = silhouette_score(Xtrain, best_k_labels), color = 'r', label = 'k cluster average silhouette score', linestyle = '--')\n","\n","y_lower = 10\n","for i in range(h):\n","    clus_i = h_sample_silhouette_values[best_h_labels == i]\n","\n","    clus_i.sort()\n","\n","    size_cluster_i = clus_i.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = cm.nipy_spectral(float(i) / h)\n","    ax[1,0].fill_betweenx(\n","        np.arange(y_lower, y_upper),\n","        0,\n","        clus_i,\n","        facecolor=color,\n","        edgecolor=color,\n","        alpha=0.7,\n","    )\n","    #ax[1,0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","    y_lower = y_upper + 0.5\n","\n","ax[1,0].axvline(x = silhouette_score(Xtrain, best_h_labels), color = 'r', label = 'hierarchy average silhouette score', linestyle = '--')\n","\n","y_lower = 10\n","for i in range(d):\n","    clus_i = d_sample_silhouette_values[best_d_labels == i]\n","\n","    clus_i.sort()\n","\n","    size_cluster_i = clus_i.shape[0]\n","    y_upper = y_lower + size_cluster_i\n","\n","    color = cm.nipy_spectral(float(i) / d)\n","    ax[2,0].fill_betweenx(\n","        np.arange(y_lower, y_upper),\n","        0,\n","        clus_i,\n","        facecolor=color,\n","        edgecolor=color,\n","        alpha=0.7,\n","    )\n","    #ax[2,0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","    y_lower = y_upper + 0.5\n","\n","ax[2,0].axvline(x = silhouette_score(Xtrain, best_d_labels), color = 'r', label = 'hierarchy average silhouette score', linestyle = '--')\n","\n","colors = cm.nipy_spectral(best_k_labels.astype(float) / k)\n","ax[0,1].scatter(\n","    Xtrain[:, 0], Xtrain[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",")\n","\n","colors = cm.nipy_spectral(best_h_labels.astype(float) / h)\n","ax[1,1].scatter(\n","    Xtrain[:, 0], Xtrain[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",")\n","\n","colors = cm.nipy_spectral(best_h_labels.astype(float) / d)\n","ax[2,1].scatter(\n","    Xtrain[:, 0], Xtrain[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",")\n","\n"],"metadata":{"id":"qtmIR-EeivPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.c linear regression -> socre is low, adding penalities might improve performance\n","plt.figure(figsize=(8, 6), dpi=80)\n","reg = LinearRegression().fit(Xtrain, ytrain)\n","yhat = reg.predict(Xtest)\n","print('Linear regression R2 score without penalities: ', reg.score(Xtrain, ytrain))\n","print('Linear regression MSE score without penalities: ', np.sum((ytest - yhat)**2)/(ytest.shape[0]))\n","# use 0.5 as threshold\n","y_hat = []\n","for i in yhat:\n","  if i < 0.5:\n","    y_hat.append(0)\n","  else:\n","    y_hat.append(1)\n","print('Linear regression F1 score without penalities: ', f1_score(ytest, y_hat))\n","\n","tn, fp, fn, tp = confusion_matrix(ytest, y_hat).ravel()\n","print(\"Linear regression percision = %f, recall = %f without penalities\" %(tp/(tp+fp), tp/(tp+fn)))\n","print('coeficients: ', reg.coef_)\n","print('intercept: ', reg.intercept_)\n","plt.scatter(np.linspace(0,10, 1000), ytest, color=\"black\")\n","plt.scatter(np.linspace(0,10, 1000), yhat)"],"metadata":{"id":"esZPbK8GX1RE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.c determine hyperparameters for elastic net\n","max_score = 0\n","aa = 0\n","ratio = 0\n","for a in range(100):\n","  for l in np.linspace(0, 1, 10):\n","    en_reg = ElasticNet(alpha = a, l1_ratio = l)\n","    en_reg.fit(Xtrain, ytrain)\n","    if en_reg.score(Xtrain, ytrain) > max_score:\n","      aa = a\n","      ratio = l\n","      max_score = en_reg.score(Xtest, ytest)"],"metadata":{"id":"QbPGXz2YctQ0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.c adding elastic net to the linear regression model doesn't improve the performance, since the data is not linear\n","l1 = aa*ratio\n","l2 = 0.5*(aa - aa*ratio)\n","\n","en_reg = ElasticNet(alpha = aa, l1_ratio = ratio)\n","en_reg.fit(Xtrain, ytrain)\n","yhat = en_reg.predict(Xtest)\n","print(\"l1 = %f, l2 = %f with score %f\" %(l1, l2, en_reg.score(Xtrain, ytrain)))\n","print('Linear regression R2 score with Elastic Net penalities: ', en_reg.score(Xtrain, ytrain))\n","print('Linear regression MSE score with Elastic Net penalities: ', np.sum((ytest - yhat)**2)/(ytest.shape[0]))\n","y_hat = []\n","for i in yhat:\n","  if i < 0.5:\n","    y_hat.append(0)\n","  else:\n","    y_hat.append(1)\n","print('Linear regression F1 score with penalities: ', f1_score(ytest, y_hat))\n","\n","tn, fp, fn, tp = confusion_matrix(ytest, y_hat).ravel()\n","print(\"Linear regression percision = %f, recall = %f with penalities\" %(tp/(tp+fp), tp/(tp+fn)))\n","\n","plt.scatter(np.linspace(0,10, 1000), ytest, color=\"black\")\n","plt.scatter(np.linspace(0,10, 1000), yhat)"],"metadata":{"id":"UpvToZQklnbh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.c SVM model\n","# determine hyperparameter: kernel\n","# use linear kernel type\n","clf = SVC(kernel='linear')\n","clf.fit(Xtrain, ytrain) \n","yhat = clf.predict(Xtest)\n","print('Linear SVM mean accuracy : ', clf.score(Xtest, ytest))\n","print('Linear SVM F1 score: ', f1_score(ytest, yhat))\n","tn, fp, fn, tp = confusion_matrix(ytest, yhat).ravel()\n","print(\"Linear SVM percision = %f, recall = %f \" %(tp/(tp+fp), tp/(tp+fn)))\n","\n","# use poly kernel type\n","clf = SVC(kernel='poly')\n","clf.fit(Xtrain, ytrain) \n","yhat = clf.predict(Xtest)\n","print('\\nPoly SVM mean accuracy : ', clf.score(Xtest, ytest))\n","yhat = clf.predict(Xtest)\n","print('Poly SVM F1 score: ', f1_score(ytest, yhat))\n","tn, fp, fn, tp = confusion_matrix(ytest, yhat).ravel()\n","print(\"Poly SVM percision = %f, recall = %f \" %(tp/(tp+fp), tp/(tp+fn)))\n","\n","clf = SVC(kernel='sigmoid')\n","clf.fit(Xtrain, ytrain) \n","yhat = clf.predict(Xtest)\n","print('\\nSigmoid SVM mean accuracy : ', clf.score(Xtest, ytest))\n","print('Sigmoid SVM F1 score: ', f1_score(ytest, yhat))\n","tn, fp, fn, tp = confusion_matrix(ytest, yhat).ravel()\n","print(\"Sigmoid SVM percision = %f, recall = %f \" %(tp/(tp+fp), tp/(tp+fn)))\n","\n","clf = SVC(kernel='rbf')\n","clf.fit(Xtrain, ytrain) \n","yhat = clf.predict(Xtest)\n","print('\\nrbf SVM mean accuracy : ', clf.score(Xtest, ytest))\n","print('rbf SVM F1 score: ', f1_score(ytest, yhat))\n","tn, fp, fn, tp = confusion_matrix(ytest, yhat).ravel()\n","print(\"rbf SVM percision = %f, recall = %f \" %(tp/(tp+fp), tp/(tp+fn)))\n","print(\"rbf kernel has the best performance\")\n","\n","plt.scatter(np.linspace(0,10, 1000), ytest, color=\"black\")\n","plt.scatter(np.linspace(0,10, 1000), yhat)"],"metadata":{"id":"QpaHDur1qcKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7a-a1Nm_2xfp"},"execution_count":null,"outputs":[]}]}